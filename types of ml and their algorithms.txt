types of ml and their algorithms
supervised learning
1- linear regression - input and output
GOAL: is to find best fit line
formula used normally is y=mx+c ( to plot, x is input, c is y intercept, y is output)
best fit line is the line which passes thru most of the data points
we use error formula to check which line can be considered as best fit line

Error= Actual-predicted
all lines we find error and which line gives least error that line is considered as least fit line
method : th data points that are not toching the data points are dragged to the best fit line(reflected onto the best fit line)
example:
actual value= 1.3
predicted value: 2
error = 1.3 - 2= -0.7
error = -1.7, etc
error= 5-3.5= +1.5
add all errors = 0.8 is cumulative error
cumulative error is taken for all lines and whichever line has least error that is considered as least fit.


Evaluation matrix for regression:

- mean square error (-2.3)^2 = positive
- root mean square error root((2.3)^2
- mean absolute error(mae)
- r2score - gives values in range of (0,1) value close to 0 bad model and
 							 near to 0.5 average model
 							 close to 1 is good model

2- Logistic regression
supervised machine learning
classification - the labelled data provided has output in terms of classes
this can be a discrete or categorical data
GOAL: to find the best separation line
separating data points by drawing a line.
**logistic regression works only when data is linearly separable. if its non linearly separable data then logistic regression cannot be applied.
non linearly separable data: we use svm, gradient boosting etc
equation for logistic regression y=w^tx
w^t distance between the data point and the separation line
whichever line has the most distance from data point that is the best segregation
sigmoid: log based function = 1/(1+e^-x) helps you classify if they data point is on positive class(1) or negative class(0)
               					helps to check is data point is properly classified or not
 	example: 50x1 + 50x1 + 50x1 + 100x0 - 50x0 - 50x0 - 50x0 = 150
 	now to check is that data point is affecting model(100x0)
 	applying sigmoid func : 1/(1+e^-150) = 1/(1-147.28)
 	range = [0,1]

Balanced and imbalanced data
0->100, 1->100 balanced data
0->100, 1->200 imbalanced data
evaluvation metrix is considered based on type of data

confusion matrix is base for eval metrics
accuracy
precision
recall
f1 score

if data is balanced check for accuracy : range 0 to 1
if the data is imbalanced for for f1score
precious and recall is used to calculate f1score

3- SVM(Support vector machine)
supervised machine learning
it supports both classification and regression but mostly used for classification
handles non linear separable data
it separates based on concept called kernals
 	linear kernel: can be classified by drawing single line
 	polynomial : takes from lower to higher dimension, checks from 1d->2d->3d... finds the dimension where data points can be easily classified.
 	RBF- radial basis func: amoeba structure.
whichever kernel gives least error that's used to create the model
here the data can be multidimensional

hyperparameter tuning(grid search cv) -> if model values isnt good then ->we give list of values It chooses kernel based on least values.
parameters -> c(regularization[0.1,0.03, etc), gamma[scale, auto]), kernal[linear, poly[degree], radial


4- decision tree
supervised machine learning
decision tree supports both regression and classification
we choose it for classification as that is more understood with this algorithm compared to regression
for regression many number of leaf nodes are required, due to continuous data hence its practically not used.
uses a hierarchical structure. 
		- root node - the start of the tree.
		- decision node - further branches nodes after the root node.
		- leaf node - last node, doesn't have classification further.
GINI index, entropy formula(either one) is used to check which value can be used as root, decision or leaf node.
the formula that model uses is unknown, we use hyper parameter tuning, whichever ever least error the model picks that formula.
GINI INDEX:  1-(p+)^2-(p-)^2
entropy: -(p-)log2(p-)-(p+)log2(p+)                                  // log2-> base is 2


gini method
	weather(5)-> sunny(2), rainy(3) | sunny(2)->yes(2), no(0) | rainy(3)-> yes(1), no(2)
more record of a certain class is considered as positive
p(rainy)(+)=3/5=0.6
p(sunny)(-)=2/5=0.4
Gini(weather)=1-(0.6)^2-(0.4)^2
		=1-0.36-0.16
		=1-0.52
		=0.48
gini(sunny)(2 record total)
	p(yes)=2/2=1
	p(no)=0/2=0
   gini(sunny)= 1-1^2-0^2
		=0
gini(rainy)
	p(yes)=1/3 =0.33
	p(no)=2/3 =0.66
   gini(rainy) = 1-(0.33)^2-(0.66)^
	       = 0.455

highest ginni value is ginni(weather). therefore that is chosen as the root node.

Entropy method
Entropy weather
	(p- ->sunny)=2/5=0.4   log(2/5)
	(p+ ->rainy)=3/5=0.6  log(3/5)

	Entropy(weather) = -(2/5)log(2/5)-(3/5)log(3/5)
			 = 0.97
Entropy(Rainy)
 	(p- ->yes)=2/3=0.66   
 	(p+ ->rainy)=1/3=0.33

 	Entropy(Rainy) = -(2/3)log(2/3)-(1/3)log(1/3)
 			 = 0.918
Entropy(Sunny)
 	Entropy(Sunny) = -(2/2)log(2/2)-(0/2)log(0/2)
 			 = 0

Entropy for weather is the highest, hence its chosen as the root node.

weighted entropy = 2/5(0)+3/5(0.918) //[record sunny + record rainy]
		 = 0.555

IG(information gain) = Entropy(parent)- Entropy(child)
		     = 0.97-0.55
		     = 0.42 // information that can be accessed only with help of parent.| IG should not be negative

Random forest:
instead of taking 1 tree for calculation purposes, we take multiple trees like 200 300 etc. to improve the model
instead of taking all 15 columns(like decision tree) it consider 2 features n provides the output.
example
	- yes->70
	- no ->30
yes is taken as final decision

Gradient Boosting
if model performance of decision tree is 0.76 to improve the accuracy we can use random forest. 
- another level to increase accuracy is - gradient boosting -> it tries fixing the error of previous tree's error
		- tree 1 -> yes(error)
		- tree 2-> same column as tree 1 and figures out why the output is faulty and redoes the calculation
disadvantage : causes overfitting (accuracy of training data is more compared to testing data) | the accurcay of training data should not be more than 0.95, as the chances of overfitting is high. therefore the threshold is 0.95
gradient boosting is slower compared to random forest and decision tree.


extreme gradient boosting
avoids the concept of overfitting.
gives good accuracy
fixes error of the previous tree
faster compared to gradient boosting.





























 